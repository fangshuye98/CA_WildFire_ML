{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version\n",
      "2.2.2\n",
      "Pyproj version\n",
      "3.6.1\n"
     ]
    }
   ],
   "source": [
    "# check python version and all packages version\n",
    "def check_python_version():\n",
    "    import sys\n",
    "    print(\"Python version\")\n",
    "    print (sys.version)\n",
    "    print(\"Pandas version\")\n",
    "    print(pd.__version__)\n",
    "    print(\"Pyproj version\")\n",
    "    print(pyproj.__version__)\n",
    "\n",
    "check_python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dead_fuel_moisture_1000h.parquet',\n",
       " 'dead_fuel_moisture_100h.parquet',\n",
       " 'max_air_temperature.parquet',\n",
       " 'max_relative_humidity.parquet',\n",
       " 'min_air_temperature.parquet',\n",
       " 'min_relative_humidity.parquet',\n",
       " 'precipitation_amount.parquet',\n",
       " 'specific_humidity.parquet',\n",
       " 'surface_downwelling_shortwave_flux.parquet',\n",
       " 'SWE.parquet',\n",
       " 'wind_from_direction.parquet',\n",
       " 'wind_speed.parquet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../Clean_Data/Weather_Data/Combined_Weather_Data_w_Veg_SubRegion_Filter'\n",
    "files = os.listdir(data_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_messages = []\n",
    "log_messages.append(\"Task: Merge weather data after veg and subregion filtering\")\n",
    "log_messages.append(f\"Processing started on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SWE.parquet as the main dataset\n",
    "all_features = pd.read_parquet(f'{data_dir}/SWE.parquet')\n",
    "# rename time to day\n",
    "all_features = all_features.rename(columns={'time': 'day'})\n",
    "log_messages.append(f\"Loaded SWE.parquet with shape: {all_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging weather files: 100%|██████████| 12/12 [23:38<00:00, 118.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read the rest of the files with a progress bar\n",
    "for file in tqdm(files, desc=\"Merging weather files\"):\n",
    "    # Read the file\n",
    "    panda_df = pd.read_parquet(f'{data_dir}/{file}')\n",
    "    # drop col year if it exists\n",
    "    if 'year' in panda_df.columns:\n",
    "        panda_df = panda_df.drop(columns=['year'])\n",
    "    if file == 'SWE.parquet':\n",
    "        # If it's the first file, we already have all_features initialized\n",
    "        continue\n",
    "    all_features = pd.merge(all_features, panda_df, on=['lon', 'lat', 'day'], how='inner')\n",
    "    log_messages.append(f\"Merged {file} with shape: {panda_df.shape} into all_features, new shape: {all_features.shape}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day                                          datetime64[ns]\n",
       "lat                                                 float64\n",
       "lon                                                 float64\n",
       "SWE                                                 float32\n",
       "year                                                  int32\n",
       "dead_fuel_moisture_1000hr                           float32\n",
       "dead_fuel_moisture_100hr                            float32\n",
       "max_air_temperature                                 float64\n",
       "max_relative_humidity                               float32\n",
       "min_air_temperature                                 float64\n",
       "min_relative_humidity                               float32\n",
       "precipitation_amount                                float32\n",
       "specific_humidity                                   float32\n",
       "surface_downwelling_shortwave_flux_in_air           float32\n",
       "wind_from_direction                                 float32\n",
       "wind_speed                                          float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../../Clean_Data/Extended_Feature_Data/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "all_features.to_parquet(f'{save_path}/Weather_Data_w_Veg_SubRegion_Filter_Merged.parquet', index=False)\n",
    "log_messages.append(f\"Saved merged weather DataFrame to {save_path}/Weather_Data_w_Veg_SubRegion_Filter_Merged.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_save_path = '../../Logs/Clean_Extended_Data/'\n",
    "# Ensure the log directory exists\n",
    "if not os.path.exists(log_save_path):\n",
    "    os.makedirs(log_save_path)\n",
    "with open(f'{log_save_path}/merge_weather_data_w_veg_subregion_log.txt', 'w') as log_file:\n",
    "    log_file.write('\\n'.join(log_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1444"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../../Clean_Data/Extended_Feature_Data/'\n",
    "file_name = 'Weather_Data_w_Veg_SubRegion_Filter_Merged.parquet'\n",
    "\n",
    "all_features = pd.read_parquet(f'{input_path}/{file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127478960, 16),\n",
       " day                                          datetime64[ns]\n",
       " lat                                                 float64\n",
       " lon                                                 float64\n",
       " SWE                                                 float32\n",
       " year                                                  int32\n",
       " dead_fuel_moisture_1000hr                           float32\n",
       " dead_fuel_moisture_100hr                            float32\n",
       " max_air_temperature                                 float64\n",
       " max_relative_humidity                               float32\n",
       " min_air_temperature                                 float64\n",
       " min_relative_humidity                               float32\n",
       " precipitation_amount                                float32\n",
       " specific_humidity                                   float32\n",
       " surface_downwelling_shortwave_flux_in_air           float32\n",
       " wind_from_direction                                 float32\n",
       " wind_speed                                          float32\n",
       " dtype: object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape, all_features.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('1994-01-01 00:00:00'), Timestamp('2020-09-30 00:00:00'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features['day'].min(), all_features['day'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day                                           0.000000\n",
       "lat                                           0.000000\n",
       "lon                                           0.000000\n",
       "SWE                                           1.709074\n",
       "year                                          0.000000\n",
       "dead_fuel_moisture_1000hr                     0.167830\n",
       "dead_fuel_moisture_100hr                      0.167830\n",
       "max_air_temperature                           0.116906\n",
       "max_relative_humidity                         0.167830\n",
       "min_air_temperature                           0.116906\n",
       "min_relative_humidity                         0.167832\n",
       "precipitation_amount                         57.369750\n",
       "specific_humidity                             0.167830\n",
       "surface_downwelling_shortwave_flux_in_air     0.167830\n",
       "wind_from_direction                           0.267880\n",
       "wind_speed                                    0.167830\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print missing rate for each column\n",
    "all_features.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the min and max dates to the log messages\n",
    "log_messages.append(f\"Minimum date in the dataset: {all_features['day'].min().strftime('%Y-%m-%d')}\")\n",
    "log_messages.append(f\"Maximum date in the dataset: {all_features['day'].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log missing rates\n",
    "missing_rates = all_features.isnull().mean() * 100\n",
    "for col, rate in missing_rates.items():\n",
    "    log_messages.append(f\"Missing rate for {col}: {rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_save_path = '../../Logs/Clean_Extended_Data/'\n",
    "# Ensure the log directory exists\n",
    "if not os.path.exists(log_save_path):\n",
    "    os.makedirs(log_save_path)\n",
    "with open(f'{log_save_path}/merge_weather_data_w_veg_subregion_log.txt', 'w') as log_file:\n",
    "    log_file.write('\\n'.join(log_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
